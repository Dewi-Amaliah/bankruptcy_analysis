---
title: "**High Dimensional Data Analysis - Group Assignment 18**"
author: "**Dewi Amaliah, Aarathy Babu, Rahul Bharadwaj & Priya Dingorkar**"
date: "9th September 2021"
output:
  bookdown::pdf_document2:
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: false
    number_sections: false
    code_folding: show
    theme: readable
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      error = FALSE, 
                      fig.align = "center")
library(tidyverse)
library(dplyr)
library(visdat)
library(splitstackshape)
library(lubridate)
library(patchwork)
library(tidyverse)
library(dplyr)
library(kableExtra)
library(FactoMineR)
library(factoextra)
library(patchwork)
library(ggrepel)
library(ggfortify)
bankruptcy <- read_rds(here::here("data/Bankruptcy.rds"))
```


\pagebreak

# Introduction

The UCLA-LoPucki Bankruptcy Research Database (BRD) is a UCLA School of Law data gathering, data linking, and data distribution initiative. The goal of the BRD is to encourage bankruptcy research by making bankruptcy data available to academic investigators worldwide. All of the data was gathered when the companies declared bankruptcy. In this study, we will use a variety of high-dimensional analysis approaches like Multidimesnionla Scaling, Principle Component Analysis and Clustering to extract useful insights from the data.


```{r, out.width=450}
knitr::include_graphics("data/bank.jpg")
```

\pagebreak

# Acknowledgement

Our sincere gratitude goes out to Ruben Loaiza-Maya and the our tutor Ari Handayani for their guidance and support. Our heartfelt thanks go to Ruben Loaiza-Maya and our instructor Ari Handayani for their advice and assistance. Their culminating efforts have placed us in a situation where we can produce this report collectively while showcasing our competence to employ diverse solutions that can be used with high dimensional data.


\pagebreak

# Data Description 

This report is based on data from US businesses that declared bankruptcy between 1980 and 2000. The information is coming from the UCLA-LoPucki Brankruptcy Research Database. Let's take a closer look at these variables and what they mean. Let's go further into the dataset to see if we can find any examples of data cleaning or wrangling.


The dataset has 436 observations and 7 variables with their description explained below.

- Name: Name of the firm
- Assets: Total assets (in millions of dollars)
- CityFiled: City where filing took place
- CPI U.S CPI at the time of filing
- DaysIn: Length of bankruptcy process
- DENYOther: CityFiled, categorized as Wilmington (DE), New York (NY) or all other cities (OT)
- Ebit: Earnings (operating income) at time of filing (in millions of dollars)
- Employees: Number of employees before bankruptcy
- EmplUnion: Number of union employees before bankruptcy
- FilingRate: Total number of other bankrupcy filings in the year of this filing
- FirmEnd: Short description of the event that ended the firm’s existence
- GDP: Gross Domestic Product for the Quarter in which the case was filed
- HeadCityPop: The population of the firms headquarters city
- HeadCourtCityToDE: The distance in miles from the firms headquarters city to the city in which
the case was filed
- HeadStAtFiling: The state in which firms headquarters is located
- Liab: Total amount of money owed (in millions of dollars)
- MonthFiled: Categorical variable where numbers from 1 to 12 correspond to months from Jan to Dec
- PrimeFiling: Prime rate of interest on the bankruptcy filing date
- Sales: Sales before bankruptcy (in dollars)
- SICMajGroup: Standard industrial clasification code
- YearFiled: Year bankruptcy was filed

Let us further examine the bankruptcy statistics. We will undertake some data analysis exploratory stages as well as data analysis, data cleansing and discussion. We will also go through the numerous approaches used for this high-dimensional data in detail later. The clean dataset is then submitted to multidimensional scales, analysis of principal components and clustering and to find about what the bankruptcy data would be most actionable. We will also examine the limitations and conclusions of our investigation.

\pagebreak



```{r}
bankruptcy <- bankruptcy %>%
  mutate(FirmEnd = ifelse(FirmEnd == "",
                          NA,
                          FirmEnd))

bankruptcy <- bankruptcy %>%
  separate(SICMajGroup,
           into = c("SIC", "SICMajGroup"),
           sep = "\\s",
           extra = "merge") %>%
  mutate(SIC = as.factor(SIC))

bankruptcy <- bankruptcy %>%
  mutate(DaysIn = ifelse(Name == "Hunt International Resources Corp.", 305,
                                    ifelse(Name == "AP Industries, Inc.", 121,
                                           ifelse(Name == "Daisy Systems Corp.", 1944,
                                                  ifelse(Name == "McCrory Corp.", 683, DaysIn)))))

bankruptcy <- bankruptcy %>%
  mutate(HeadCourtCityToDE = ifelse(Name == "Divi Hotels, N.V.", 1126,
                                    ifelse(Name == "Loewen Group, Inc.", 2942,
                                           ifelse(Name == "Philip Services Corp. (1999)", 1234,
                                                  HeadCourtCityToDE))))

bankruptcy <- bankruptcy %>%
  mutate(Employees = ifelse(Name == "County Seat, Inc.", 5180, Employees))

bankruptcy <- bankruptcy %>%
  mutate(Sales = ifelse(Name == "County Seat, Inc.", 1645170944, Sales))

bank_data_op1 <- bankruptcy %>%
  filter(!is.na(Ebit) & !is.na(Liab)) %>%
  filter(Name != "Promus Companies Inc. (Harrahs Jazz Co. only)")

data_clean <- bank_data_op1 %>%
  select(-FirmEnd, -EmplUnion) %>%
  mutate(DENYOther = as.factor(DENYOther),
         MonthFiled = as.factor(MonthFiled),
         YearFiled = as.factor(YearFiled))
```



# Princple Component Analysis (PCA)

Now that we've seen how to input this high-dimensional data into Multidimensional scaling (MDS) to obtain a low (typically 2) dimensional representation. Let us now perform a Principal Component Analysis (PCA), which is a dimensional-reduction method that is frequently used to reduce the dimensional of large data sets by transforming a large set of variables into a smaller one that still contains the majority of the information in the large set.

```{r}
data_clean%>%
  select_if(is.numeric)%>%
  prcomp(scale. = TRUE)->pca
```


- Let's carry out PCA on our bankruptcy data. Lets investigate if our data is a good fit for PCA. Let's us further investigate how the variables in our data are correlated and many PC's explain the variation of our data.

- Before we apply the this principle to our data, it is very important we standardized our variables as this ensures that results are not sensitive to the units of measurement. Thus giving us more accurate analysis.

```{r}
summary(pca)
```



- Using the `summary` function we can infer the following:
  + Proportion of variance explained by the first four PCs together is 73.28%
  + Proportion of variance explained by the first and second PC alone is 30.10% and 23.64% respectively
  + Using kaisers rule, choose those PC's whose variance and standard deviation greater than 1, in our bankruptcy data we will choose 4 PC's.


- Let us now plot use the scree plot to find the total number of PC's that best explain our data.

```{r}
screeplot(pca,type="lines")
```

- Using the scree plot we infer that our bankruptcy data is explained by the first three PC's. Also note this is different to kaisers rule.

- Let us now plot a biplot, that will help us infer intersting features about our data. A PCA biplot shows both PC scores of samples (dots) and loadings of variables(vectors).

```{r biplot_bank, fig.cap="PCA Biplot"}
data_clean <- data_clean %>%
  mutate(abbreviation=abbreviate(Name))

rownames(pca$x)<-pull(data_clean,abbreviation)

autoplot(pca,label = TRUE, 
         label.size = 3.6,
         loadings = TRUE, 
         loadings.colour = 'blue',
         loadings.label = TRUE, 
         loadings.label.size = 4.5) +
  labs(title = "Biplot on Bankruptcy Dataset") +
  theme_bw()
```

- Looking at figure \@ref(fig:biplot_bank), more closely we infer that there is one company that is an outlier to our analysis. PCA allows us to infer that since this observation is very far apart from our variables direction it is best to remove this observation from our data and fit PCA again.

- On further examination on this outlier we infer that Texaco Inc. which comes under Petroleum SIC and Refining And Related Industries is the outlier in our bankruptcy data. This also agrees with our MDS method where we saw the same observations. Let us omit this observation and fit our PCA again.



## PCA without outlier

```{r}
nooutlier <- bank_data_op1 %>%
    mutate(abb=abbreviate(Name))

nooutlier <- nooutlier%>%
  dplyr::select(c(abb,Assets, CPI, Employees, Ebit, Liab, FilingRate, GDP, PrimeFiling, Sales))%>%
  dplyr::filter(abb!="TIn.")

nooutlier%>%
  select_if(is.numeric)%>%
  prcomp(scale. = TRUE)->pca_no_out

summary(pca_no_out)
```

- As seen earlier similarly using the `summary` function we can infer the following:
  + Proportion of variance explained by the first four PCs together is now 84.25%
  + Proportion of variance explained by the first and second PC alone is 33.72% and 29.23% respectively
  + Using kaisers rule, choose those PC's whose variance and standard deviation greater than 1, now we will choose 3 PC's.


- Let us now plot use the scree plot to find the total number of PC's that best explain our data.

```{r}
screeplot(pca_no_out,type="lines")
```

- We can now say that scree plot and the kaiser's rule agree with each other thus this helps in confirming that most of the variations is captures by the first three principal components from our dataset.

- As we can see from the summary of our PCA that our third PC explains approx 11% of variation in our data but we cannot visualize this with a biplot. Nevertheless we still plot the biplot for our first two PC's as they explain around 62.95% of variation in our data.


```{r biplot_bank_no_out, fig.cap="PCA Biplot without Outlier"}
rownames(pca_no_out$x)<-pull(nooutlier,abb)

autoplot(pca_no_out,label = TRUE, label.size = 2.5,
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3) +
  labs(title = "Biplot on Bankruptcy without Outlier") +
  theme_bw()
```


- Looking at the figure \@ref(fig:biplot_bank_no_out) we can infer that:
  + We can see the various spread of the different companies before filing for bankruptcy.
  + The further away these vectors are from a PC origin, the more influence they have on that PC. For instance taking a closer a look at the left quadrant, we infer that CPI has more influence, followed by GDP and FilingRate.
  + We also that variables 90° angle indicates no correlation between them, In our data we can infer that Employees with GDP/CPI shows an angle of 90° thus showing no correlation
  + We also infer that variables with 180° angle indicates negative correlation. In our case we can say that PrimeFiling is negative correlated with GDP, CPI and FilingRate making an almost 180° angle.
  + Similarly,variables with angle close to 0° indicates positive correlation. In our dataset we can see that CPI and GDP are highly positively correlated, Assest, Liab, Ebit abd Sales are all positively correlated as the angle between all them is nearly zero.



```{r corr_plot, fig.cap="Correlation Plot"}
var <- get_pca_var(pca_no_out)
fviz_pca_var(pca_no_out, col.var = "black") +
  labs(title = "Correlation Plot") +
  theme_bw()
```

- We can also refer to figure \@ref(fig:corr_plot) for more clear visuals of the angles between variables making it for the audience to distinguish between the variables that are positively, negatively or not correlated at all























